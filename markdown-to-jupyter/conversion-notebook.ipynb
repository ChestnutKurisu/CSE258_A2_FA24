{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-14T17:59:16.681370Z",
     "start_time": "2024-11-14T17:59:16.664122Z"
    }
   },
   "source": [
    "import nbformat\n",
    "from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell\n",
    "\n",
    "def markdown_to_jupyter(markdown_text):\n",
    "    nb = new_notebook()\n",
    "    lines = markdown_text.split('\\n')\n",
    "    current_cell = []\n",
    "    in_code_block = False\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.startswith('```'):\n",
    "            if in_code_block:\n",
    "                # End of code block\n",
    "                nb.cells.append(new_code_cell('\\n'.join(current_cell)))\n",
    "                current_cell = []\n",
    "                in_code_block = False\n",
    "            else:\n",
    "                # Start of code block\n",
    "                if current_cell:\n",
    "                    nb.cells.append(new_markdown_cell('\\n'.join(current_cell)))\n",
    "                    current_cell = []\n",
    "                in_code_block = True\n",
    "        else:\n",
    "            current_cell.append(line)\n",
    "    \n",
    "    # Add any remaining content as a markdown cell\n",
    "    if current_cell:\n",
    "        nb.cells.append(new_markdown_cell('\\n'.join(current_cell)))\n",
    "    \n",
    "    return nb\n",
    "\n",
    "# Example usage\n",
    "markdown_text = r'''\n",
    "# Climate Tech Fundraisers Data Scraping and Processing\n",
    "\n",
    "This notebook demonstrates the process of scraping, cleaning, processing, and performing feature engineering on a climate tech fundraiser dataset. The data is collected from various newsletters and blogs, then cleaned and enriched with additional information for further analysis.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Data Collection and Cleaning](#data-collection-and-cleaning)\n",
    "   - [Data Crawling and Scraping](#data-crawling-and-scraping)\n",
    "   - [Data Extraction with GPT-4](#data-extraction-with-gpt-4)\n",
    "3. [Deduplication and Location Standardization](#deduplication-and-location-standardization)\n",
    "   - [Deduplication of Fundraiser Entities](#deduplication-of-fundraiser-entities)\n",
    "   - [Fetching Addresses Using Selenium](#fetching-addresses-using-selenium)\n",
    "   - [Location Standardization and Geolocation](#location-standardization-and-geolocation)\n",
    "   - [Extracting Telephone Numbers](#extracting-telephone-numbers)\n",
    "   - [Country and Continent Information](#country-and-continent-information)\n",
    "4. [Outlier Mitigation and Final Steps](#outlier-mitigation-and-final-steps)\n",
    "5. [Data Display Preparation](#data-display-preparation)\n",
    "\n",
    "---\n",
    "\n",
    "<a id='introduction'></a>\n",
    "## 1. Introduction\n",
    "\n",
    "In this notebook, we will create a dataset of climate technology fundraisers from March 2020 to January 2024 by scraping newsletters and blogs published by various sources. The dataset captures key investments, innovative startups, and pivotal fundraising events driving solutions to environmental challenges.\n",
    "\n",
    "---\n",
    "\n",
    "<a id='data-collection-and-cleaning'></a>\n",
    "## 2. Data Collection and Cleaning\n",
    "\n",
    "<a id='data-crawling-and-scraping'></a>\n",
    "### 2.1 Data Crawling and Scraping\n",
    "\n",
    "We will use Selenium to crawl and scrape newsletters and blogs from climate websites. This generic code snippet can be adapted for any newsletter website.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from currency_converter import CurrencyConverter\n",
    "from io import StringIO\n",
    "import openai\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import dateparser\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<Your-OpenAI-API-Key>\"\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize data structures\n",
    "website_page_html = dict()\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.keepcool.co/'\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Go to the webpage\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load and extract links\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a[href]'))\n",
    ")\n",
    "link_elements = driver.find_elements(By.CSS_SELECTOR, 'a[href]')\n",
    "links = [element.get_attribute('href') for element in link_elements]\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Filter links to match certain criteria\n",
    "post_links = [l for l in links if f'{url}/p' in l]\n",
    "\n",
    "# Function to process HTML tags\n",
    "def process_tag(tag):\n",
    "    try:\n",
    "        # If the tag is a link, return it as is\n",
    "        if tag.name == \"a\":\n",
    "            return str(tag)\n",
    "        # If the tag contains other tags, process each child\n",
    "        if tag.find('a') or tag.find('href'):\n",
    "            return ''.join(process_tag(child) if child else '' for child in tag.children)\n",
    "        # For text elements, return the text\n",
    "        return tag.string if tag.string else ''\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "# Scrape individual posts\n",
    "driver = webdriver.Chrome(service=service)\n",
    "posts = dict()\n",
    "\n",
    "for link in post_links:\n",
    "    print('Processing:', link)\n",
    "\n",
    "    # Go to the post page\n",
    "    driver.get(link)\n",
    "\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # Extract information\n",
    "    organization_name = soup.find('meta', property='og:site_name')['content']\n",
    "    title = soup.find('title').text\n",
    "    authors = ''.join(soup.select_one('.bh__byline_wrapper').get_text('\\n').split('\\n')[:-1])\n",
    "    date_posted = soup.select_one('.bh__byline_wrapper .text-wt-text-on-background').text\n",
    "    content_blocks = soup.select('#content-blocks p')\n",
    "\n",
    "    # Process content blocks\n",
    "    content_with_html = []\n",
    "    for block in content_blocks:\n",
    "        processed_block = process_tag(block)\n",
    "        content_with_html.append(f'<p>{processed_block}</p>')\n",
    "\n",
    "    content = '\\n'.join(content_with_html)\n",
    "    posts[link] = {\n",
    "        'title': title,\n",
    "        'authors': authors,\n",
    "        'date_posted': date_posted,\n",
    "        'content': content\n",
    "    }\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "website_page_html['Keep Cool'] = {'url': url, 'posts': posts}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<a id='data-extraction-with-gpt-4'></a>\n",
    "### 2.2 Data Extraction with GPT-4\n",
    "\n",
    "We use GPT-4 to extract fundraiser information from the scraped content and transform it into a tabular format.\n",
    "\n",
    "```python\n",
    "# Helper functions and regex patterns\n",
    "def contains_fundraising_keywords(text):\n",
    "    keywords = [\"$\", \"€\", \"million\", \"billion\", \"dollar\"]\n",
    "    text_lower = text.lower()\n",
    "    return any(keyword in text_lower for keyword in keywords)\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"\n",
    "    u\"\\U0001F300-\\U0001F5FF\"\n",
    "    u\"\\U0001F680-\\U0001F6FF\"\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "    u\"\\U00002500-\\U00002BEF\"\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "    u\"\\U0001f926-\\U0001f937\"\n",
    "    u\"\\U00010000-\\U0010ffff\"\n",
    "    u\"\\u2640-\\u2642\"\n",
    "    u\"\\u2600-\\u2B55\"\n",
    "    u\"\\u200d\"\n",
    "    u\"\\u23cf\"\n",
    "    u\"\\u23e9\"\n",
    "    u\"\\u231a\"\n",
    "    u\"\\ufe0f\"\n",
    "    u\"\\u3030\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# Define the prompt for GPT-4\n",
    "fundraising_analysis_prompt = \"\"\"I want you to act as a data analyst. Analyze the newsletter below and consolidate data in a tabular format. The columns in the table should be the company or agency raising the funds or making the donation, the amount raised or donated, the date the fundraiser was published or the funding was made available as outlined in the newsletter, a brief description of the fundraiser or the work done by the company, and a list of links to the fundraiser company's website or the article talking about the funding. Output only the table and no other text. Finally, a column to indicate the sector each fundraising company falls under. Choose from one of the following climate (clean) technology sectors: \n",
    "\n",
    "Built Environment: Making our buildings more energy efficient and reliable\n",
    "Carbon Technology: How we can lower the concentration of CO2 in Earth's atmosphere\n",
    "Energy and Grid: Electricity powers everything — It's time to electrify\n",
    "Food and Agriculture: Regenerative agriculture and ways to sustainably produce food\n",
    "Industry and Manufacturing: Decarbonize the entire lifecycle of the production of goods\n",
    "Intelligence and Adaptation: Climate risk and data that help us adapt to the effects of climate change\n",
    "Supporting Catalysts: Businesses that indirectly support decarbonization efforts\n",
    "Transportation and Mobility: alternative mobility and electrifying how we move people and things\n",
    "Other: not fitting in any category above\"\"\"\n",
    "\n",
    "newsletter_prompt_template = \"\"\"Title of newsletter: <TITLE>.\n",
    "Newsletter date: <NEWSLETTER_DATE>.\n",
    "Newsletter content: <NEWSLETTER_CONTENT>.\"\"\"\n",
    "\n",
    "# Process each newsletter\n",
    "fundraisers = dict()\n",
    "\n",
    "for publication_firm in website_page_html:\n",
    "    fundraisers[publication_firm] = dict()\n",
    "\n",
    "    for newsletter_link in website_page_html[publication_firm]['posts']:\n",
    "        print('Processing:', publication_firm, newsletter_link)\n",
    "\n",
    "        newsletter_post = website_page_html[publication_firm]['posts'][newsletter_link]\n",
    "        newsletter_content = newsletter_post['content']\n",
    "        content_paragraphs = newsletter_content.replace('</p><p>', '</p>\\n<p>').split('\\n')\n",
    "        fundraising_paragraphs = [p for p in content_paragraphs if contains_fundraising_keywords(p)]\n",
    "        newsletter_fundraising_content = '\\n'.join(fundraising_paragraphs)\n",
    "\n",
    "        # Clean and prepare the content\n",
    "        soup = BeautifulSoup(newsletter_fundraising_content, 'html.parser')\n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.decompose()\n",
    "\n",
    "        newsletter_prompt = fundraising_analysis_prompt + emoji_pattern.sub(\n",
    "            r'', newsletter_prompt_template.replace('<TITLE>', newsletter_post['title'])\n",
    "            .replace('<NEWSLETTER_DATE>', newsletter_post['date_posted'])\n",
    "            .replace('<NEWSLETTER_CONTENT>', str(soup))\n",
    "        )\n",
    "        newsletter_prompt = newsletter_prompt.replace(' class=\"link\"', '').replace(' target=\"_blank\"', '').replace(\n",
    "            ' rel=\"noopener noreferrer nofollow\"', '').replace(\n",
    "            ' style=\"-webkit-text-decoration:underline #5c14d9;color:#5c14d9;font-style:italic;text-decoration:underline #5c14d9;\"', ''\n",
    "        ).replace('<strong>', '').replace('</strong>', '').replace('<blockquote>', '').replace('</blockquote>', '')\n",
    "\n",
    "        # Call GPT-4 API\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": newsletter_prompt}]\n",
    "        )\n",
    "        fundraisers[publication_firm][newsletter_link] = response.choices[0].message.content\n",
    "\n",
    "# Consolidate the extracted data into a DataFrame\n",
    "extended_fr_list = []\n",
    "for publication_firm in fundraisers:\n",
    "    firm_url = website_page_html[publication_firm]['url']\n",
    "    for newsletter_link in fundraisers[publication_firm].keys():\n",
    "        newsletter_post = website_page_html[publication_firm]['posts'][newsletter_link]\n",
    "        fr_df = pd.read_csv(StringIO(fundraisers[publication_firm][newsletter_link]), sep='|').dropna(axis=1, how='all').iloc[1:]\n",
    "        fr_df = fr_df[[col for col in fr_df.columns if not col.startswith('Unnamed:')]]\n",
    "        if len(fr_df.columns) == 7:\n",
    "            fr_df.drop(list(fr_df.columns)[1], axis=1, inplace=True)\n",
    "        extended_fr_list.extend([\n",
    "            [\n",
    "                publication_firm, firm_url, newsletter_post['title'], newsletter_link, newsletter_post['authors'],\n",
    "                newsletter_post['date_posted'], *list(row)[1:]\n",
    "            ] for row in fr_df.itertuples()\n",
    "        ])\n",
    "\n",
    "fundraisers_df = pd.DataFrame(\n",
    "    data=extended_fr_list,\n",
    "    columns=[\n",
    "        'name_of_newsletter_firm', 'url_link_to_firms_website', 'title_of_newsletter', 'link_to_newsletter',\n",
    "        'names_of_newsletter_authors', 'newsletter_date', 'fundraising_entity', 'amount_raised',\n",
    "        'date_of_funding_reported', 'fundraiser_description', 'link_to_fundraising_announcement', 'clean_technology_sector'\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<a id='deduplication-and-location-standardization'></a>\n",
    "## 3. Deduplication and Location Standardization\n",
    "\n",
    "<a id='deduplication-of-fundraiser-entities'></a>\n",
    "### 3.1 Deduplication of Fundraiser Entities\n",
    "\n",
    "We perform deduplication using fuzzy string matching.\n",
    "\n",
    "```python\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "def deduplicate(records):\n",
    "    seen = dict()\n",
    "    for record in records:\n",
    "        if record is None:\n",
    "            continue\n",
    "        if len(seen) == 0:\n",
    "            seen[record] = record\n",
    "            continue\n",
    "        # Check if similar record is already seen\n",
    "        match, score, _ = process.extractOne(record, seen.keys(), scorer=fuzz.WRatio)\n",
    "\n",
    "        if score < 93:  # Threshold of 93%\n",
    "            seen[record] = record\n",
    "        elif score < 100:\n",
    "            seen[record] = match\n",
    "            print(f\"'{record}' matched with '{match}' at score {score}\")\n",
    "\n",
    "    return seen\n",
    "\n",
    "# Apply deduplication\n",
    "unique_companies_dict = deduplicate(sorted(fundraisers_df['fundraising_entity'].unique()))\n",
    "fundraisers_df['fundraising_entity'] = fundraisers_df['fundraising_entity'].replace(unique_companies_dict)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<a id='fetching-addresses-using-selenium'></a>\n",
    "### 3.2 Fetching Addresses Using Selenium\n",
    "\n",
    "We scrape addresses from online databases like CB Insights.\n",
    "\n",
    "```python\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Setup WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "def fetch_headquarters_location_with_selenium(company_name):\n",
    "    try:\n",
    "        # Navigate to the company's CB Insights page\n",
    "        url = f\"https://www.cbinsights.com/company/{company_name}\"\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the page to load\n",
    "        driver.implicitly_wait(2)\n",
    "\n",
    "        # Find the 'Headquarters Location' element\n",
    "        header = driver.find_element(By.XPATH, \"//h2[contains(text(), 'Headquarters Location')]\")\n",
    "        address = header.find_element(By.XPATH, \"./following-sibling::address\")\n",
    "\n",
    "        location = address.text\n",
    "        return location.replace('\\n', ', ').replace(',,', ',')\n",
    "    except:\n",
    "        return 'Address not found'\n",
    "\n",
    "fundraiser_loc_dict = dict()\n",
    "\n",
    "for entity in fundraisers_df['fundraising_entity'].unique():\n",
    "    loc = fetch_headquarters_location_with_selenium(\n",
    "        entity.lower().replace(' ', '-').replace('.', '-').strip(' °')\n",
    "    )\n",
    "    if loc not in (\"Failed to retrieve search results.\", \"Address not found\", \"\"):\n",
    "        fundraiser_loc_dict[entity] = loc\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "fundraisers_df['Location'] = fundraisers_df['fundraising_entity'].map(fundraiser_loc_dict)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<a id='location-standardization-and-geolocation'></a>\n",
    "### 3.3 Location Standardization and Geolocation\n",
    "\n",
    "We standardize company locations and convert them into GPS coordinates using GeoPy.\n",
    "\n",
    "```python\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "\n",
    "def standardize_location(row):\n",
    "    if row['Location'] != 'N/A':\n",
    "        try:\n",
    "            location = geolocator.geocode(row['Location'], exactly_one=True, timeout=10)\n",
    "            if location:\n",
    "                return location.address\n",
    "            else:\n",
    "                return row['Location']\n",
    "        except GeocoderTimedOut:\n",
    "            return row['Location']\n",
    "    else:\n",
    "        return row['Location']\n",
    "\n",
    "def get_geolocation(row):\n",
    "    if row['Standardized location'] != 'N/A':\n",
    "        try:\n",
    "            location = geolocator.geocode(row['Standardized location'], exactly_one=True, timeout=10)\n",
    "            if location:\n",
    "                return (location.latitude, location.longitude)\n",
    "        except GeocoderTimedOut:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Apply standardization\n",
    "fundraisers_df['Standardized location'] = fundraisers_df.apply(standardize_location, axis=1)\n",
    "fundraisers_df['GeoLocation'] = fundraisers_df.apply(get_geolocation, axis=1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<a id='extracting-telephone-numbers'></a>\n",
    "### 3.4 Extracting Telephone Numbers\n",
    "\n",
    "We extract telephone numbers from the standardized locations.\n",
    "\n",
    "```python\n",
    "def extract_telephone(address):\n",
    "    # Regex pattern to match phone numbers\n",
    "    pattern = re.compile(r'[\\s,]*(?:\\+?\\d{1,3}[\\s-]?)?(?:\\(\\d{1,3}\\)[\\s-]?)?\\d{1,4}[\\s-]?\\d{1,4}[\\s-]?\\d{1,4}(?:[\\s-]?\\d{1,4})?[\\s-]?\\d{1,4}$')\n",
    "    match = pattern.search(address)\n",
    "    if match:\n",
    "        phone = match.group()\n",
    "        cleaned_address = pattern.sub('', address).strip()\n",
    "        return cleaned_address, phone\n",
    "    return address, None\n",
    "\n",
    "fundraisers_df['Standardized location'], fundraisers_df['Telephone'] = zip(\n",
    "    *fundraisers_df['Standardized location'].apply(extract_telephone)\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<a id='country-and-continent-information'></a>\n",
    "### 3.5 Country and Continent Information\n",
    "\n",
    "We use GPS coordinates to get country and continent information using `pycountry`.\n",
    "\n",
    "```python\n",
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "\n",
    "def get_continent(country_name):\n",
    "    try:\n",
    "        country_alpha2 = pycountry.countries.get(name=country_name).alpha_2\n",
    "        continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n",
    "        continent_name = pc.convert_continent_code_to_continent_name(continent_code)\n",
    "        return continent_name\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing country name '{country_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "def get_location_info(row):\n",
    "    country, continent = None, None\n",
    "    if row['GeoLocation'] is not None:\n",
    "        try:\n",
    "            location = geolocator.reverse(row['GeoLocation'], exactly_one=True)\n",
    "            address = location.raw['address']\n",
    "            country_code = address.get('country_code', '').upper()\n",
    "            if country_code:\n",
    "                country = pycountry.countries.get(alpha_2=country_code).name\n",
    "                continent = get_continent(country)\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {row.name}: {e}\")\n",
    "    return pd.Series([country, continent])\n",
    "\n",
    "fundraisers_df[['Country', 'Continent']] = fundraisers_df.apply(get_location_info, axis=1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<a id='outlier-mitigation-and-final-steps'></a>\n",
    "## 4. Outlier Mitigation and Final Steps\n",
    "\n",
    "We omitted records with unrealistic amounts or irrelevant data. We manually reviewed records with significant discrepancies to ensure location accuracy. Clean technology sectors were also inferred using zero-shot text classification to ensure consistency.\n",
    "\n",
    "---\n",
    "\n",
    "<a id='data-display-preparation'></a>\n",
    "## 5. Data Display Preparation\n",
    "\n",
    "We prepare the DataFrame for display by selecting relevant columns and cleaning data.\n",
    "\n",
    "```python\n",
    "def create_climate_fundraisers_data_display_df(climate_fundraisers_df):\n",
    "    data_display_df = climate_fundraisers_df[\n",
    "        [\n",
    "            'fundraising_entity', 'fundraiser_description', 'date_of_funding_reported', 'amount_raised',\n",
    "            'normalized_amount_raised', 'clean_technology_sector', 'Standardized location', 'Country',\n",
    "            'Telephone', 'link_to_fundraising_announcement', 'title_of_newsletter', 'link_to_newsletter',\n",
    "            'names_of_newsletter_authors', 'newsletter_date', 'name_of_newsletter_firm', 'url_link_to_firms_website'\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Clean links and amounts\n",
    "    data_display_df['link_to_fundraising_announcement'] = data_display_df['link_to_fundraising_announcement'].apply(\n",
    "        lambda x: max(re.findall(r'\\(https?://[^\\s)]+\\)', x), key=len).strip('()') if '[' in x else None\n",
    "    )\n",
    "    data_display_df['normalized_amount_raised'] = data_display_df['normalized_amount_raised'].apply(\n",
    "        lambda x: int(x[1:]) if x != 'N/A' else None\n",
    "    )\n",
    "    data_display_df['link_to_newsletter'] = data_display_df['link_to_newsletter'] + \"?ref=\" + data_display_df['title_of_newsletter']\n",
    "    data_display_df.drop('title_of_newsletter', axis=1, inplace=True)\n",
    "    data_display_df['url_link_to_firms_website'] = data_display_df['url_link_to_firms_website'] + \"?ref=\" + data_display_df['name_of_newsletter_firm']\n",
    "\n",
    "    return data_display_df\n",
    "\n",
    "# Create the display DataFrame\n",
    "data_display_df = create_climate_fundraisers_data_display_df(fundraisers_df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** Ensure all required libraries are installed and replace `<Your-OpenAI-API-Key>` with your actual OpenAI API key. Adjust code snippets as needed based on actual data and website structures.\n",
    "'''"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T17:59:17.303208Z",
     "start_time": "2024-11-14T17:59:17.266156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "notebook = markdown_to_jupyter(markdown_text)\n",
    "with open('output_notebook.ipynb', 'w') as f:\n",
    "    nbformat.write(notebook, f)"
   ],
   "id": "29b248b3e383e3d1",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f7503c44e7f2f372"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
